#!/usr/bin/env python3
"""
Enhanced test suite for the Employee Onboarding Assistant
Based on the existing test approach but with improved coverage and error handling
"""

from chatbot import OnboardingAssistant
import json
import time

def test_onboarding_scenarios():
    """Test  onboarding scenarios with better validation"""
    print("ü§ñ Testing Enhanced Employee Onboarding Assistant")
    print("=" * 70)
    
    # Initialize the assistant
    try:
        assistant = OnboardingAssistant()
        print("‚úÖ Assistant initialized successfully")
    except Exception as e:
        print(f"‚ùå Failed to initialize assistant: {e}")
        return False
    
    # Enhanced test scenarios covering real onboarding use cases
    test_scenarios = [
        {
            "category": "üë• Team Information",
            "queries": [
                "Who is Quinn Hernandez?",
                "Show me all team members in the DevOps department",
                "Who are the Security Engineers in the company?",
                "List all members in the Engineering department",
                "Find information about Sarah Johnson"
            ]
        },
        {
            "category": "üìã Project Information", 
            "queries": [
                "What projects are currently active?",
                "What is the Employee Onboarding Assistant project about?",
                "Tell me about project AIAE001",
                "Which projects are in the Engineering department?",
                "What's the status of the Customer Portal project?"
            ]
        },
        {
            "category": "üíª Technology Stack",
            "queries": [
                "What technologies do we use for frontend development?",
                "What database technologies do we use?",
                "What AI/ML tools are available in our projects?",
                "Tell me about our backend technology stack",
                "What cloud platforms do we use?"
            ]
        },
        {
            "category": "‚öôÔ∏è Development Processes",
            "queries": [
                "Tell me about our development processes",
                "What is our code review process?",
                "How do we handle sprint planning?",
                "What's our deployment process?",
                "Tell me about our testing procedures"
            ]
        },
        {
            "category": "üîç Personal Assignment",
            "queries": [
                "What project am I assigned to?",
                "Who is my manager?",
                "What team will I be working with?",
                "What's my role in the project?",
                "When did I start working here?"
            ]
        },
        {
            "category": "üåü Natural Language Queries",
            "queries": [
                "I'm new here, can you help me understand our team structure?",
                "Where can I find documentation for our technology stack?",
                "Who should I contact about project assignments?",
                "I need to learn about our development workflow",
                "Help me understand what tools I'll be using"
            ]
        },
        {
            "category": "‚ö†Ô∏è Error Handling & Edge Cases",
            "queries": [
                "Find information about John Doe",  # Non-existent user
                "What's the tech stack for project XYZ?",  # Non-existent project
                "",  # Empty query
                "a" * 100,  # Very long query
                "Tell me about department ABC"  # Non-existent department
            ]
        },
        {
            "category": "üö´ Out-of-Scope Queries",
            "queries": [
                "What's my salary?",
                "How do I book vacation time?",
                "My laptop is broken, who do I contact?",
                "What are the office hours?",
                "How do I submit an expense report?"
            ]
        }
    ]
    
    total_queries = 0
    successful_responses = 0
    errors = 0
    out_of_scope_handled = 0
    
    print("\nüîç Testing  Onboarding Scenarios:")
    print("-" * 50)
    
    for scenario in test_scenarios:
        print(f"\n{scenario['category']}")
        print("‚îÄ" * len(scenario['category']))
        
        for i, query in enumerate(scenario['queries'], 1):
            total_queries += 1
            print(f"\n{i}. Query: {query}")
            
            if not query.strip():
                print("   ‚ö†Ô∏è  Skipping empty query")
                continue
                
            print("   Processing...", end=" ")
            
            try:
                start_time = time.time()
                response = assistant.send_message(query)
                response_time = time.time() - start_time
                
                # Validate response quality
                if not response:
                    print("‚ùå Empty response")
                    errors += 1
                elif len(response) < 10:
                    print("‚ö†Ô∏è  Very short response")
                    print(f"   Response: {response}")
                elif "error" in response.lower() and "system error" in response.lower():
                    print("‚ùå System error detected")
                    errors += 1
                elif any(keyword in response.lower() for keyword in ["contact hr", "contact it", "out of scope"]):
                    print("‚úÖ Out-of-scope handled correctly")
                    out_of_scope_handled += 1
                    successful_responses += 1
                else:
                    print("‚úÖ Good response")
                    successful_responses += 1
                
                print(f"   Time: {response_time:.2f}s")
                print(f"   Length: {len(response)} chars")
                
                # Show first 100 characters of response
                preview = response.replace('\n', ' ')[:100]
                print(f"   Preview: {preview}...")
                
            except Exception as e:
                print(f"‚ùå Exception: {e}")
                errors += 1
    
    # Performance and conversation summary
    print(f"\nüìä Test Results Summary:")
    print("‚îÄ" * 30)
    summary = assistant.get_conversation_summary()
    
    print(f"Total queries tested: {total_queries}")
    print(f"Successful responses: {successful_responses}")
    print(f"Errors encountered: {errors}")
    print(f"Out-of-scope handled: {out_of_scope_handled}")
    print(f"Success rate: {(successful_responses/total_queries*100):.1f}%")
    
    print(f"\nConversation Statistics:")
    print(f"Total messages: {summary['total_messages']}")
    print(f"User messages: {summary['user_messages']}")
    print(f"Assistant responses: {summary['assistant_messages']}")
    print(f"Information lookups: {summary['tool_calls']}")
    
    return successful_responses > total_queries * 0.8  # 80% success rate

def test_data_loading():
    """Enhanced test for data loading with better error detection"""
    print("\nüìÅ Testing  Data Loading:")
    print("-" * 40)
    
    from chatbot import get_member_information, get_process_information, get_techstack_information, get_user_project_assignment
    
    test_results = {}
    
    # Test member information
    try:
        print("Testing member information loading...", end=" ")
        member_data = get_member_information()
        
        if "error" in member_data.lower():
            print("‚ö†Ô∏è  Error detected in member data")
            test_results["member_info"] = "error"
        else:
            member_json = json.loads(member_data)
            total_members = sum(len(p.get('members', [])) for p in member_json)
            print(f"‚úÖ Loaded: {len(member_json)} projects, {total_members} members")
            test_results["member_info"] = "success"
            
    except Exception as e:
        print(f"‚ùå Failed: {e}")
        test_results["member_info"] = "failed"
    
    # Test process information
    try:
        print("Testing process information loading...", end=" ")
        process_data = get_process_information()
        
        if "error" in process_data.lower():
            print("‚ö†Ô∏è  Error detected in process data")
            test_results["process_info"] = "error"
        else:
            process_json = json.loads(process_data)
            total_processes = sum(len(p.get('processes', [])) for p in process_json)
            print(f"‚úÖ Loaded: {len(process_json)} projects, {total_processes} processes")
            test_results["process_info"] = "success"
            
    except Exception as e:
        print(f"‚ùå Failed: {e}")
        test_results["process_info"] = "failed"
    
    # Test tech stack information
    try:
        print("Testing tech stack information loading...", end=" ")
        tech_data = get_techstack_information()
        
        if "error" in tech_data.lower():
            print("‚ö†Ô∏è  Error detected in tech stack data")
            test_results["tech_info"] = "error"
        else:
            tech_json = json.loads(tech_data)
            print(f"‚úÖ Loaded: {len(tech_json)} projects with tech stacks")
            test_results["tech_info"] = "success"
            
    except Exception as e:
        print(f"‚ùå Failed: {e}")
        test_results["tech_info"] = "failed"
    
    # Test user project assignment with different scenarios
    test_users = [
        ("Quinn Hernandez", "Known user"),
        ("John Smith", "Common name"),
        ("Sarah", "First name only"),
        ("NonExistent User", "Invalid user"),
        ("", "Empty name")
    ]
    
    print("\nTesting user project assignment lookup:")
    assignment_results = []
    
    for user_name, description in test_users:
        if not user_name:
            continue
            
        try:
            print(f"  Testing {description} ({user_name})...", end=" ")
            result = get_user_project_assignment(user_name)
            result_json = json.loads(result)
            
            if result_json.get("user_found"):
                projects = result_json.get("total_projects", 0)
                print(f"‚úÖ Found {projects} project(s)")
                assignment_results.append("found")
            else:
                print("‚úÖ Not found (handled correctly)")
                assignment_results.append("not_found")
                
        except Exception as e:
            print(f"‚ùå Failed: {e}")
            assignment_results.append("failed")
    
    # Summary
    print(f"\nüìä Data Loading Test Results:")
    for test_name, result in test_results.items():
        status_icon = "‚úÖ" if result == "success" else "‚ö†Ô∏è" if result == "error" else "‚ùå"
        print(f"  {status_icon} {test_name}: {result}")
    
    successful_tests = sum(1 for result in test_results.values() if result == "success")
    total_tests = len(test_results)
    
    return successful_tests == total_tests

def test_assistant_robustness():
    """Test assistant robustness and error recovery"""
    print("\nüõ°Ô∏è  Testing Assistant Robustness:")
    print("-" * 35)
    
    try:
        assistant = OnboardingAssistant()
        
        # Test conversation reset
        print("Testing conversation reset...", end=" ")
        initial_length = assistant.get_conversation_length()
        assistant.reset_conversation()
        reset_length = assistant.get_conversation_length()
        
        if reset_length > 0:
            print("‚úÖ Reset successful")
        else:
            print("‚ùå Reset failed")
            
        # Test conversation management
        print("Testing conversation management...", end=" ")
        assistant.set_additional_context("I work in Engineering department")
        
        summary = assistant.get_conversation_summary()
        if summary.get("total_messages", 0) > 0:
            print("‚úÖ Context management working")
        else:
            print("‚ùå Context management failed")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Robustness test failed: {e}")
        return False

def main():
    """Run all enhanced tests"""
    print("üß™ Enhanced Employee Onboarding Assistant Test Suite")
    print("=" * 80)
    
    test_results = {}
    
    # Run data loading tests
    test_results["data_loading"] = test_data_loading()

    # Run robustness tests
    test_results["robustness"] = test_assistant_robustness()
    
    # Run  scenario tests
    test_results["scenarios"] = test_onboarding_scenarios()
    
    # Final summary
    print("\n" + "=" * 80)
    print("üéØ Final Test Results:")
    print("-" * 25)
    
    for test_name, result in test_results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"  {status}: {test_name.replace('_', ' ').title()}")
    
    all_passed = all(test_results.values())
    
    if all_passed:
        print("\nüéâ All tests passed! The system is working well.")
        print("\nüí° Recommended next steps:")
        print("   1. Test with the Streamlit app using these queries")
        print("   2. Validate responses with domain experts")
        print("   3. Test with real Azure OpenAI API integration")
        print("   4. Conduct user acceptance testing")
        print("   5. Monitor performance in production")
    else:
        print("\n‚ö†Ô∏è  Some tests failed. Please review the issues above.")
        print("   Check your data files and configuration.")
    
    return all_passed

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
